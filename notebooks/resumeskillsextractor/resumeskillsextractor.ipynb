{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\r\n",
      "  Downloading seqeval-0.0.12.tar.gz (21 kB)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.18.1)\r\n",
      "Requirement already satisfied: Keras>=2.2.4 in /opt/conda/lib/python3.7/site-packages (from seqeval) (2.4.0)\r\n",
      "Requirement already satisfied: tensorflow>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval) (2.2.0)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval) (2.10.0)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval) (1.4.1)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval) (5.3.1)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.11.2)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.1.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (2.2.0)\r\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.6.3)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.14.0)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (0.9.0)\r\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.29.0)\r\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (2.2.2)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (3.2.1)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.1.2)\r\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (3.12.2)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (0.2.0)\r\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (0.34.2)\r\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (0.3.3)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (3.2.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (2.23.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.6.0.post3)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.0.1)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.14.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (0.4.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (2020.4.5.2)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (2.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.24.3)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (4.0)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (3.1.1)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (1.2.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->Keras>=2.2.4->seqeval) (3.0.1)\r\n",
      "Building wheels for collected packages: seqeval\r\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7423 sha256=affdbef1a57032c9d3426db61b3f08e2829f2de5a155a054aafa0a208205b526\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/dc/cc/62/a3b81f92d35a80e39eb9b2a9d8b31abac54c02b21b2d466edc\r\n",
      "Successfully built seqeval\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-0.0.12\r\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "EPOCHS = 6\n",
    "MODEL_PATH = '../input/bert-base-uncased'\n",
    "TOKENIZER = BertTokenizerFast('../input/bert-base-uncased/vocab.txt', lowercase=True)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('../input/resume-entities-for-ner/Entity Recognition in Resumes.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha\\nApplication Development Associat...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar\\nActive member of IIIT Committe...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina\\nHyderabad, Telangana - ...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai\\nOperational Analyst (SQL DBA) En...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Abhishek Jha\\nApplication Development Associat...   \n",
       "1  Afreen Jamadar\\nActive member of IIIT Committe...   \n",
       "2  Akhil Yadav Polemaina\\nHyderabad, Telangana - ...   \n",
       "3  Alok Khandai\\nOperational Analyst (SQL DBA) En...   \n",
       "4  Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...   \n",
       "\n",
       "                                          annotation  extras  \n",
       "0  [{'label': ['Skills'], 'points': [{'start': 12...     NaN  \n",
       "1  [{'label': ['Email Address'], 'points': [{'sta...     NaN  \n",
       "2  [{'label': ['Skills'], 'points': [{'start': 37...     NaN  \n",
       "3  [{'label': ['Skills'], 'points': [{'start': 80...     NaN  \n",
       "4  [{'label': ['Degree'], 'points': [{'start': 20...     NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content'].replace(\"\\n\", \" \")\n",
    "            entities = []\n",
    "            data_annotations = data['annotation']\n",
    "            if data_annotations is not None:\n",
    "                for annotation in data_annotations:\n",
    "                    #only a single point in text annotation.\n",
    "                    point = annotation['points'][0]\n",
    "                    labels = annotation['label']\n",
    "                    # handle both list of labels or a single label.\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    for label in labels:\n",
    "                        point_start = point['start']\n",
    "                        point_end = point['end']\n",
    "                        point_text = point['text']\n",
    "                        \n",
    "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                        if lstrip_diff != 0:\n",
    "                            point_start = point_start + lstrip_diff\n",
    "                        if rstrip_diff != 0:\n",
    "                            point_end = point_end - rstrip_diff\n",
    "                        entities.append((point_start, point_end + 1 , label))\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trim_entity_spans(convert_dataturks_to_spacy('../input/resume-entities-for-ner/Entity Recognition in Resumes.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(offset, labels):\n",
    "    if offset[0] == 0 and offset[1] == 0:\n",
    "        return 'O'\n",
    "    for label in labels:\n",
    "        if offset[1] >= label[0] and offset[0] <= label[1]:\n",
    "            return label[2]\n",
    "    return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\",\"Skills\",\"College Name\",\"Email Address\",\"Designation\",\"Companies worked at\",\"Graduation Year\",\"Years of Experience\",\"Location\"]\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i:t for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume(data, tokenizer, tag2idx, max_len, is_test=False):\n",
    "    tok = tokenizer.encode_plus(data[0], max_length=max_len, return_offsets_mapping=True)\n",
    "    curr_sent = {'orig_labels':[], 'labels': []}\n",
    "    \n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    \n",
    "    if not is_test:\n",
    "        labels = data[1]['entities']\n",
    "        labels.reverse()\n",
    "        for off in tok['offset_mapping']:\n",
    "            label = get_label(off, labels)\n",
    "            curr_sent['orig_labels'].append(label)\n",
    "            curr_sent['labels'].append(tag2idx[label])\n",
    "        curr_sent['labels'] = curr_sent['labels'] + ([0] * padding_length)\n",
    "    \n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + ([0] * padding_length)\n",
    "    return curr_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeDataset(Dataset):\n",
    "    def __init__(self, resume, tokenizer, tag2idx, max_len, is_test=False):\n",
    "        self.resume = resume\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.resume)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = process_resume(self.resume[idx], self.tokenizer, self.tag2idx, self.max_len, self.is_test)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(data['input_ids'], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(data['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(data['labels'], dtype=torch.long),\n",
    "            'orig_label': data['orig_labels']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(data)\n",
    "train_data, val_data = data[:180], data[180:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = ResumeDataset(train_data, TOKENIZER, tag2idx, MAX_LEN)\n",
    "val_d = ResumeDataset(val_data, TOKENIZER, tag2idx, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_d)\n",
    "train_dl = DataLoader(train_d, sampler=train_sampler, batch_size=8)\n",
    "\n",
    "val_dl = DataLoader(val_d, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(model, ff):\n",
    "\n",
    "    # ff: full_finetuning\n",
    "    if ff:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters())\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_tokens(tokenizer, tag2idx):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    pad_tok = vocab[\"[PAD]\"]\n",
    "    sep_tok = vocab[\"[SEP]\"]\n",
    "    cls_tok = vocab[\"[CLS]\"]\n",
    "    o_lab = tag2idx[\"O\"]\n",
    "\n",
    "    return pad_tok, sep_tok, cls_tok, o_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot_confusion_matrix(valid_tags, pred_tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Create an annotated confusion matrix by adding label\n",
    "    annotations and formatting to sklearn's `confusion_matrix`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create header from unique tags\n",
    "    header = sorted(list(set(valid_tags + pred_tags)))\n",
    "\n",
    "    # Calculate the actual confusion matrix\n",
    "    matrix = confusion_matrix(valid_tags, pred_tags, labels=header)\n",
    "\n",
    "    # Final formatting touches for the string output\n",
    "    mat_formatted = [header[i] + \"\\t\\t\\t\" + str(row) for i, row in enumerate(matrix)]\n",
    "    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(valid_tags, pred_tags):\n",
    "    return (np.array(valid_tags) == np.array(pred_tags)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(MODEL_PATH, num_labels=len(tag2idx))\n",
    "model.to(DEVICE);\n",
    "optimizer_grouped_parameters = get_hyperparameters(model, True)\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GRAD_NORM = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    idx2tag,\n",
    "    tag2idx,\n",
    "    max_grad_norm,\n",
    "    device,\n",
    "    train_dataloader,\n",
    "    valid_dataloader\n",
    "):\n",
    "\n",
    "    pad_tok, sep_tok, cls_tok, o_lab = get_special_tokens(tokenizer, tag2idx)\n",
    "    \n",
    "    epoch = 0\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        epoch += 1\n",
    "\n",
    "        # Training loop\n",
    "        print(\"Starting training loop.\")\n",
    "        model.train()\n",
    "        tr_loss, tr_accuracy = 0, 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        tr_preds, tr_labels = [], []\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Add batch to gpu\n",
    "            \n",
    "            # batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            loss, tr_logits = outputs[:2]\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Compute train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "            preds_mask = (\n",
    "                (b_input_ids != cls_tok)\n",
    "                & (b_input_ids != pad_tok)\n",
    "                & (b_input_ids != sep_tok)\n",
    "            )\n",
    "\n",
    "            tr_logits = tr_logits.cpu().detach().numpy()\n",
    "            tr_label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "            preds_mask = preds_mask.cpu().detach().numpy()\n",
    "            tr_batch_preds = np.argmax(tr_logits[preds_mask.squeeze()], axis=1)\n",
    "            tr_batch_labels = tr_label_ids.to(\"cpu\").numpy()\n",
    "            tr_preds.extend(tr_batch_preds)\n",
    "            tr_labels.extend(tr_batch_labels)\n",
    "\n",
    "            # Compute training accuracy\n",
    "            tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n",
    "            tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                parameters=model.parameters(), max_norm=max_grad_norm\n",
    "            )\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        tr_loss = tr_loss / nb_tr_steps\n",
    "        tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "\n",
    "        # Print training loss and accuracy per epoch\n",
    "        print(f\"Train loss: {tr_loss}\")\n",
    "        print(f\"Train accuracy: {tr_accuracy}\")\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Validation loop\n",
    "        \"\"\" \n",
    "        print(\"Starting validation loop.\")\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        predictions, true_labels = [], []\n",
    "\n",
    "        for batch in valid_dataloader:\n",
    "\n",
    "            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    labels=b_labels,\n",
    "                )\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "            preds_mask = (\n",
    "                (b_input_ids != cls_tok)\n",
    "                & (b_input_ids != pad_tok)\n",
    "                & (b_input_ids != sep_tok)\n",
    "            )\n",
    "\n",
    "            logits = logits.cpu().detach().numpy()\n",
    "            label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "            preds_mask = preds_mask.cpu().detach().numpy()\n",
    "            val_batch_preds = np.argmax(logits[preds_mask.squeeze()], axis=1)\n",
    "            val_batch_labels = label_ids.to(\"cpu\").numpy()\n",
    "            predictions.extend(val_batch_preds)\n",
    "            true_labels.extend(val_batch_labels)\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(val_batch_labels, val_batch_preds)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        # Evaluate loss, acc, conf. matrix, and class. report on devset\n",
    "        pred_tags = [idx2tag[i] for i in predictions]\n",
    "        valid_tags = [idx2tag[i] for i in true_labels]\n",
    "        cl_report = classification_report(valid_tags, pred_tags)\n",
    "        conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "\n",
    "        # Report metrics\n",
    "        print(f\"Validation loss: {eval_loss}\")\n",
    "        print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "        print(f\"Classification Report:\\n {cl_report}\")\n",
    "        print(f\"Confusion Matrix:\\n {conf_mat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop.\n",
      "Train loss: 0.9121079846568729\n",
      "Train accuracy: 0.7761088067106691\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  17%|█▋        | 1/6 [00:12<01:04, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.591684103012085\n",
      "Validation Accuracy: 0.8294253597808664\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Skills       0.00      0.00      0.00       869\n",
      "        Designation       0.00      0.00      0.00        89\n",
      "               Name       0.00      0.00      0.00        41\n",
      "           Location       0.00      0.00      0.00        54\n",
      "      Email Address       0.76      0.80      0.78      1130\n",
      "       College Name       0.00      0.00      0.00        33\n",
      "    Graduation Year       0.00      0.00      0.00        16\n",
      "Companies worked at       0.00      0.00      0.00        59\n",
      "             Degree       0.00      0.00      0.00        35\n",
      "Years of Experience       0.00      0.00      0.00         5\n",
      "\n",
      "          micro avg       0.76      0.39      0.51      2331\n",
      "          macro avg       0.37      0.39      0.38      2331\n",
      "\n",
      "Confusion Matrix:\n",
      " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n",
      "College Name\t\t\t[  0   0   0   0   0   0   0   0 208   0   0]\n",
      "Companies worked at\t\t\t[  0   0   0   0   4   0   0   0 232   0   0]\n",
      "Degree\t\t\t[  0   0   0   0   0   0   0   0 152   0   0]\n",
      "Designation\t\t\t[  0   0   0   0   2   0   0   0 321   0   0]\n",
      "Email Address\t\t\t[  0   0   0   0 908   0   0   0 222   0   0]\n",
      "Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\n",
      "Location\t\t\t[  0   0   0   0   2   0   0   0 173   0   0]\n",
      "Name\t\t\t[  0   0   0   0   5   0   0   9 176   0   0]\n",
      "O\t\t\t[    0     0     0     0   272     0     0     0 12436     0     0]\n",
      "Skills\t\t\t[  0   0   0   0   0   0   0   0 869   0   0]\n",
      "Years of Experience\t\t\t[ 0  0  0  0  0  0  0  0 15  0  0]\n",
      "Starting training loop.\n",
      "Train loss: 0.4377691266329392\n",
      "Train accuracy: 0.8733936210045805\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 2/6 [00:24<00:50, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4092428505420685\n",
      "Validation Accuracy: 0.8697032689668168\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Skills       0.82      0.44      0.57       869\n",
      "        Designation       0.11      0.04      0.06        92\n",
      "               Name       0.34      0.56      0.43        41\n",
      "           Location       0.02      0.02      0.02        53\n",
      "      Email Address       0.74      0.91      0.81      1130\n",
      "       College Name       0.00      0.00      0.00        33\n",
      "    Graduation Year       0.00      0.00      0.00        16\n",
      "Companies worked at       0.08      0.03      0.05        60\n",
      "             Degree       0.00      0.00      0.00        35\n",
      "Years of Experience       0.00      0.00      0.00         5\n",
      "\n",
      "          micro avg       0.68      0.62      0.65      2334\n",
      "          macro avg       0.67      0.62      0.62      2334\n",
      "\n",
      "Confusion Matrix:\n",
      " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n",
      "College Name\t\t\t[ 61   0   0   0   0   0   0   7 140   0   0]\n",
      "Companies worked at\t\t\t[  1  37   0   5   1   0   0   7 194   0   0]\n",
      "Degree\t\t\t[65  0  4  1  0  0  0  1 81  0  0]\n",
      "Designation\t\t\t[  0   1   0  55   0   0   0  20 238   0   0]\n",
      "Email Address\t\t\t[   0    0    0    0 1025    0    0    0  105    0    0]\n",
      "Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\n",
      "Location\t\t\t[  0   0   0   0   0   0  53   4 118   0   0]\n",
      "Name\t\t\t[  0   0   0   0   0   0   1 188   1   0   0]\n",
      "O\t\t\t[   36     7     0     1   365     0     9    24 12179    87     0]\n",
      "Skills\t\t\t[  0   0   0   0   0   0   0   0 484 385   0]\n",
      "Years of Experience\t\t\t[ 0  0  0  0  0  0  0  0 15  0  0]\n",
      "Starting training loop.\n",
      "Train loss: 0.2971187058998191\n",
      "Train accuracy: 0.9058106427164977\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 3/6 [00:37<00:37, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.37221054136753084\n",
      "Validation Accuracy: 0.8554678214530224\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Skills       0.41      0.73      0.53       869\n",
      "        Designation       0.22      0.18      0.20        89\n",
      "               Name       0.84      0.90      0.87        41\n",
      "           Location       0.51      0.54      0.52        54\n",
      "      Email Address       0.77      0.90      0.83      1130\n",
      "       College Name       0.16      0.24      0.19        33\n",
      "    Graduation Year       0.00      0.00      0.00        16\n",
      "Companies worked at       0.14      0.24      0.18        59\n",
      "             Degree       0.15      0.14      0.14        35\n",
      "Years of Experience       0.00      0.00      0.00         5\n",
      "\n",
      "          micro avg       0.55      0.75      0.64      2331\n",
      "          macro avg       0.57      0.75      0.64      2331\n",
      "\n",
      "Confusion Matrix:\n",
      " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n",
      "College Name\t\t\t[131   0   3   0   0   0   1   0  73   0   0]\n",
      "Companies worked at\t\t\t[  4 127   1   7   0   0   2   3  92   0   0]\n",
      "Degree\t\t\t[15  0 83  1  0  0  0  0 53  0  0]\n",
      "Designation\t\t\t[  0  36   0 156   0   0   0   2 125   4   0]\n",
      "Email Address\t\t\t[   0    0    0    0 1017    0    0    0  113    0    0]\n",
      "Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\n",
      "Location\t\t\t[  1   0   0   0   0   0 110   0  64   0   0]\n",
      "Name\t\t\t[  0   1   0   1   0   0   0 187   1   0   0]\n",
      "O\t\t\t[   52    85     8    16   299     0    31     5 11322   890     0]\n",
      "Skills\t\t\t[  0   0   0   0   0   0   0   0 238 631   0]\n",
      "Years of Experience\t\t\t[ 0  0  0  1  0  0  0  0 14  0  0]\n",
      "Starting training loop.\n",
      "Train loss: 0.2213073703257934\n",
      "Train accuracy: 0.9274862168881586\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 4/6 [00:49<00:24, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3218224748969078\n",
      "Validation Accuracy: 0.8899422432687878\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Skills       0.61      0.64      0.62       869\n",
      "        Designation       0.38      0.36      0.37        92\n",
      "               Name       0.80      0.90      0.85        41\n",
      "           Location       0.51      0.60      0.55        53\n",
      "      Email Address       0.82      0.85      0.84      1130\n",
      "       College Name       0.19      0.24      0.21        33\n",
      "    Graduation Year       0.00      0.00      0.00        16\n",
      "Companies worked at       0.20      0.30      0.24        60\n",
      "             Degree       0.29      0.23      0.25        35\n",
      "Years of Experience       0.00      0.00      0.00         5\n",
      "\n",
      "          micro avg       0.68      0.71      0.69      2334\n",
      "          macro avg       0.68      0.71      0.69      2334\n",
      "\n",
      "Confusion Matrix:\n",
      " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n",
      "College Name\t\t\t[134   0   3   0   0   0   1   0  70   0   0]\n",
      "Companies worked at\t\t\t[  5 133   0   8   0   0   1   4  94   0   0]\n",
      "Degree\t\t\t[16  0 93  0  0  0  0  0 43  0  0]\n",
      "Designation\t\t\t[  0  19   0 203   0   0   0   2  90   0   0]\n",
      "Email Address\t\t\t[  0   0   0   0 963   0   0   0 167   0   0]\n",
      "Graduation Year\t\t\t[ 0  0  1  0  0  0  0  0 18  0  0]\n",
      "Location\t\t\t[  0   0   0   0   0   0 125   0  50   0   0]\n",
      "Name\t\t\t[  0   0   0   1   0   0   0 188   1   0   0]\n",
      "O\t\t\t[   58    81     4    34   210     0    48     7 11905   361     0]\n",
      "Skills\t\t\t[  0   1   0   0   0   0   0   0 312 556   0]\n",
      "Years of Experience\t\t\t[ 0  0  0  1  0  0  0  0 14  0  0]\n",
      "Starting training loop.\n",
      "Train loss: 0.1753910928964615\n",
      "Train accuracy: 0.9443425535072218\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  83%|████████▎ | 5/6 [01:01<00:12, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3967111185193062\n",
      "Validation Accuracy: 0.8574283177569489\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Skills       0.42      0.79      0.55       869\n",
      "        Designation       0.30      0.35      0.32        89\n",
      "               Name       0.83      0.93      0.87        41\n",
      "           Location       0.53      0.72      0.61        54\n",
      "      Email Address       0.82      0.86      0.84      1130\n",
      "       College Name       0.25      0.42      0.31        33\n",
      "    Graduation Year       0.00      0.00      0.00        16\n",
      "Companies worked at       0.16      0.36      0.22        59\n",
      "             Degree       0.46      0.54      0.50        35\n",
      "Years of Experience       0.00      0.00      0.00         5\n",
      "\n",
      "          micro avg       0.56      0.78      0.65      2331\n",
      "          macro avg       0.61      0.78      0.67      2331\n",
      "\n",
      "Confusion Matrix:\n",
      " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n",
      "College Name\t\t\t[170   0   8   0   0   0   2   0  28   0   0]\n",
      "Companies worked at\t\t\t[  2 172   3  13   0   0   0   3  43   0   0]\n",
      "Degree\t\t\t[ 10   0 129   2   0   0   0   0  11   0   0]\n",
      "Designation\t\t\t[  0  37   0 227   0   0   0   1  51   7   0]\n",
      "Email Address\t\t\t[  0   0   0   0 968   0   0   0 162   0   0]\n",
      "Graduation Year\t\t\t[ 0  0  1  0  0  0  0  0 18  0  0]\n",
      "Location\t\t\t[  1   0   0   0   0   0 143   0  31   0   0]\n",
      "Name\t\t\t[  0   0   0   3   0   0   0 187   0   0   0]\n",
      "O\t\t\t[   97   170    25    78   213     0    56     8 11116   945     0]\n",
      "Skills\t\t\t[  0   1   0   0   0   0   0   0 179 689   0]\n",
      "Years of Experience\t\t\t[ 0  0  0  2  0  0  0  0 12  0  1]\n",
      "Starting training loop.\n",
      "Train loss: 0.14045068945573724\n",
      "Train accuracy: 0.9559719479928784\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 6/6 [01:13<00:00, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4421268403530121\n",
      "Validation Accuracy: 0.8616699157194712\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Skills       0.43      0.73      0.54       869\n",
      "        Designation       0.30      0.25      0.27        92\n",
      "               Name       0.88      0.93      0.90        41\n",
      "           Location       0.64      0.64      0.64        53\n",
      "      Email Address       0.82      0.80      0.81      1130\n",
      "       College Name       0.25      0.30      0.27        33\n",
      "    Graduation Year       1.00      0.06      0.12        16\n",
      "Companies worked at       0.17      0.32      0.22        60\n",
      "             Degree       0.57      0.49      0.52        35\n",
      "Years of Experience       1.00      0.20      0.33         5\n",
      "\n",
      "          micro avg       0.57      0.72      0.64      2334\n",
      "          macro avg       0.62      0.72      0.65      2334\n",
      "\n",
      "Confusion Matrix:\n",
      " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n",
      "College Name\t\t\t[118   0   8   0   0   0   0   0  82   0   0]\n",
      "Companies worked at\t\t\t[  3 151   3   7   0   0   0   2  79   0   0]\n",
      "Degree\t\t\t[  3   0 108   0   0   0   0   0  41   0   0]\n",
      "Designation\t\t\t[  0  39   0 173   0   0   0   1  97   4   0]\n",
      "Email Address\t\t\t[  0   0   0   0 901   0   0   0 229   0   0]\n",
      "Graduation Year\t\t\t[ 0  0  0  0  0  1  0  0 18  0  0]\n",
      "Location\t\t\t[  0   1   0   0   0   0 125   0  49   0   0]\n",
      "Name\t\t\t[  0   0   0   2   0   0   0 186   2   0   0]\n",
      "O\t\t\t[   50   104     8    26   203     0    29     0 11447   841     0]\n",
      "Skills\t\t\t[  0   1   0   0   0   0   0   0 233 635   0]\n",
      "Years of Experience\t\t\t[ 0  0  0  0  0  0  0  0 11  0  4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_save_model(\n",
    "    model, \n",
    "    TOKENIZER, \n",
    "    optimizer, \n",
    "    EPOCHS, \n",
    "    idx2tag, \n",
    "    tag2idx, \n",
    "    MAX_GRAD_NORM, \n",
    "    DEVICE, \n",
    "    train_dl, \n",
    "    val_dl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": EPOCHS,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    'model_e6.tar',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
